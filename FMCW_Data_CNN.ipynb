{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FMCW_Data_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNri3mmjWlXLA8HfYsEGVRh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertege/FMCW-Data-Classification-/blob/main/FMCW_Data_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN4ixvH39HW6"
      },
      "source": [
        "# Sadece 'fast' dataset 1 kat augmente ediliyor.\n",
        "# Mimari küçültüldü.\n",
        "# Bir önceki commite'e göre farklı olarak dropout_prob_cnn 0.1 oldu.\n",
        "# Augmentation ve Normalization fonksiyona çevrildi.\n",
        "# 10 defa run edildi ve değerler aşağıda paylaşıldı.\n",
        "# Mean test accuracy is 0.87, mean test f1 score is 0.84, max test accuracy is 0.89, max test f1 score is 0.86,\n",
        "# Min test accuracy is 0.85, min test f1 score is 0.82, std of test accuracy is 0.01, std of test f1 score is 0.02\n",
        "# Time elapsed through all process: 662.07, sec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzJwN6GfkN8Z"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, Normalization\n",
        "import tensorflow as tf\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from numpy.random import seed\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import time\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK4xoT2T8ztd",
        "outputId": "1ba3d55d-e669-483e-dbc4-49bda9d2d3de"
      },
      "source": [
        "pip install mat73"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mat73\n",
            "  Downloading mat73-0.52-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from mat73) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mat73) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->mat73) (1.5.2)\n",
            "Installing collected packages: mat73\n",
            "Successfully installed mat73-0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FERVnWrkrwE",
        "outputId": "4fbbcd70-64a7-45d1-cf55-775a25df4c3c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o19JHieQ81cS"
      },
      "source": [
        "import mat73\n",
        "spectrograms_shuffled = mat73.loadmat('/content/drive/MyDrive/spectrograms_shuffled.mat')\n",
        "spectrograms_shuffled = spectrograms_shuffled['spectrograms_shuffled']\n",
        "spectrograms_shuffled = np.transpose(spectrograms_shuffled, (2, 0, 1))\n",
        "spectrograms_label_shuffled = scipy.io.loadmat('/content/drive/MyDrive/spectrograms_label_shuffled.mat')\n",
        "spectrograms_label_shuffled = spectrograms_label_shuffled['spectrograms_label_shuffled']  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFp_PErnMLN2"
      },
      "source": [
        "# Downscale Images\n",
        "downscaled_size_x = 51\n",
        "downscaled_size_y = 422\n",
        "spectrograms_shuffled_downscaled = np.zeros((spectrograms_shuffled.shape[0],downscaled_size_x,downscaled_size_y))\n",
        "for ii in range(spectrograms_shuffled.shape[0]):\n",
        "  aa = np.float32(spectrograms_shuffled[ii,:,:])\n",
        "  img = cv2.cvtColor(aa, cv2.COLOR_GRAY2BGR)\n",
        "  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  spectrograms_shuffled_downscaled[ii,:,:] = cv2.resize(img_gray, dsize=(downscaled_size_y, downscaled_size_x), interpolation=cv2.INTER_CUBIC)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAqV2GQAPIGR"
      },
      "source": [
        "del spectrograms_shuffled"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St3aorsY__Y6",
        "outputId": "a726fb96-622b-4d58-e9ed-15447c5b8855"
      },
      "source": [
        "# Add new axis\n",
        "print(spectrograms_shuffled_downscaled.shape[1:])\n",
        "spectrograms_shuffled_downscaled = spectrograms_shuffled_downscaled[:,:,:,np.newaxis] \n",
        "print(spectrograms_shuffled_downscaled.shape[1:])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(51, 422)\n",
            "(51, 422, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKyYWwKB2Q7E",
        "outputId": "d9cd923b-289a-4277-9898-2e5b6cfa1db6"
      },
      "source": [
        "for ii in range(1):\n",
        "  print(ii)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNpkBBIadpPo"
      },
      "source": [
        "def split_and_augmentation_of_training(spectrograms_shuffled_downscaled_train,spectrograms_label_shuffled_train, augmentation_enable):\n",
        "  # ---------------- Parameters ----------------\n",
        "  repeat_of_augmentation_for_fast = 1\n",
        "  repeat_of_augmentation_for_slow = np.floor(repeat_of_augmentation_for_fast/2)\n",
        "  repeat_of_augmentation_for_slow = int(repeat_of_augmentation_for_slow)\n",
        "  slow_size_of_validation = 20\n",
        "  fast_size_of_validation = slow_size_of_validation/2\n",
        "  fast_size_of_validation = int(fast_size_of_validation)\n",
        "  slow_indexes = np.where(spectrograms_label_shuffled_train == 0)[0]\n",
        "  fast_indexes = np.delete(range(0, spectrograms_label_shuffled_train.shape[0]), slow_indexes)\n",
        "  slow_spectrograms_train_val = spectrograms_shuffled_downscaled_train[slow_indexes,:,:,:]\n",
        "  fast_spectrograms_train_val = spectrograms_shuffled_downscaled_train[fast_indexes,:,:,:]\n",
        "  # ---------------- Seperate Validation From Training Data ----------------\n",
        "  # -- Slow --\n",
        "  randomlist_for_validation_indx = random.sample(range(0, slow_spectrograms_train_val.shape[0]), slow_size_of_validation) # split validation data\n",
        "  randomlist_for_train_indx = np.delete(range(0, slow_spectrograms_train_val.shape[0]), randomlist_for_validation_indx) # split training data\n",
        "  slow_spectrograms_train = slow_spectrograms_train_val[randomlist_for_train_indx,:,:,:]\n",
        "  slow_spectrograms_val = slow_spectrograms_train_val[randomlist_for_validation_indx,:,:,:]\n",
        "  slow_label_val = np.zeros((slow_size_of_validation,1))\n",
        "  size_of_samples_slow = slow_spectrograms_train.shape[0]\n",
        "  # -- Fast -- \n",
        "  randomlist_for_validation_indx = random.sample(range(0, fast_spectrograms_train_val.shape[0]), fast_size_of_validation) # split validation data\n",
        "  randomlist_for_train_indx = np.delete(range(0, fast_spectrograms_train_val.shape[0]), randomlist_for_validation_indx) # split training data\n",
        "  fast_spectrograms_train = fast_spectrograms_train_val[randomlist_for_train_indx,:,:,:]\n",
        "  fast_spectrograms_val = fast_spectrograms_train_val[randomlist_for_validation_indx,:,:,:]\n",
        "  fast_label_val = np.ones((fast_size_of_validation,1))\n",
        "  size_of_samples_fast = fast_spectrograms_train.shape[0]\n",
        "  # -- Concat Fast and Slow Data at Validation Dataset -- \n",
        "  validation_spectrograms = np.concatenate((fast_spectrograms_val,slow_spectrograms_val),axis=0)\n",
        "  validation_labels = np.concatenate((fast_label_val,slow_label_val),axis=0)\n",
        "  if augmentation_enable == True: \n",
        "    # ---------------- Augmente Train Data for Fast ----------------\n",
        "    augmented_image_fast = np.zeros((size_of_samples_fast*repeat_of_augmentation_for_fast,fast_spectrograms_train.shape[1],fast_spectrograms_train.shape[2],1))\n",
        "    spectrograms_fast_label = np.ones((size_of_samples_fast*(repeat_of_augmentation_for_fast+1),1))\n",
        "    for jj in range(repeat_of_augmentation_for_fast):\n",
        "      for ii in range(size_of_samples_fast):\n",
        "        augmented_image_fast[size_of_samples_fast*jj+ii,:,:,:] = data_augmentation(fast_spectrograms_train[ii,:,:,:])\n",
        "    augmented_image_fast = np.concatenate((augmented_image_fast,fast_spectrograms_train),axis=0)   \n",
        "    # ---------------- Augmente Train Data for Slow ----------------\n",
        "    augmented_image_slow = np.zeros((size_of_samples_slow*repeat_of_augmentation_for_slow,slow_spectrograms_train.shape[1],slow_spectrograms_train.shape[2],1))\n",
        "    spectrograms_slow_label = np.zeros((size_of_samples_slow*(repeat_of_augmentation_for_slow+1),1))\n",
        "    if repeat_of_augmentation_for_slow == 0:\n",
        "      augmented_image_slow = slow_spectrograms_train\n",
        "    else:\n",
        "      for kk in range(repeat_of_augmentation_for_slow):\n",
        "        for ii in range(size_of_samples_slow):\n",
        "          augmented_image_slow[size_of_samples_slow*kk+ii,:,:,:] = data_augmentation(slow_spectrograms_train[ii,:,:,:])\n",
        "      augmented_image_slow = np.concatenate((augmented_image_slow,slow_spectrograms_train),axis=0)    \n",
        "  else:\n",
        "    augmented_image_fast = fast_spectrograms_train\n",
        "    augmented_image_slow = slow_spectrograms_train\n",
        "    spectrograms_fast_label = np.ones((size_of_samples_fast,1))\n",
        "    spectrograms_slow_label = np.zeros((size_of_samples_slow,1))\n",
        "  return (augmented_image_fast,augmented_image_slow,spectrograms_fast_label,spectrograms_slow_label,validation_spectrograms,validation_labels)\n",
        "\n",
        "\n",
        "def normalize_inputs(spectrograms_shuffled_downscaled_test, validation_spectrograms, augmented_image, normalize_inputs_enable):\n",
        "  # ---------------- Normalize Inputs ----------------\n",
        "  if normalize_inputs_enable == True:\n",
        "    layer = Normalization(axis=None)\n",
        "    layer.adapt(spectrograms_shuffled_downscaled_test)\n",
        "    spectrograms_shuffled_downscaled_test = layer(spectrograms_shuffled_downscaled_test)\n",
        "    layer = Normalization(axis=None)\n",
        "    layer.adapt(validation_spectrograms)\n",
        "    validation_spectrograms = layer(validation_spectrograms)\n",
        "    layer = Normalization(axis=None)\n",
        "    layer.adapt(augmented_image)\n",
        "    augmented_image = layer(augmented_image)\n",
        "  else:\n",
        "    (spectrograms_shuffled_downscaled_test, validation_spectrograms, augmented_image) = (spectrograms_shuffled_downscaled_test, validation_spectrograms, augmented_image)\n",
        "  return(spectrograms_shuffled_downscaled_test, validation_spectrograms, augmented_image)\n",
        "\n",
        "t = time.time()\n",
        "# ---------------- Augmente and shuffle (train and test) data data ----------------\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.1),\n",
        "])\n",
        "# ---------------- Parameters ----------------\n",
        "augmentation_enable = True\n",
        "normalize_inputs_enable = True\n",
        "num_folds = 6\n",
        "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state = None) # random_state = 1 ile split run'dan run'a sabit.\n",
        "test_accuracy_per_run = []\n",
        "f1_score_per_run = []\n",
        "epoch_number = 100\n",
        "batch_size = 32\n",
        "dropout_prob_cnn = 0.1\n",
        "dropout_prob_dense = 0.5\n",
        "number_of_repeat = 5\n",
        "for repeat_run_number in range(number_of_repeat):\n",
        "  test_accuracy_per_fold = []\n",
        "  f1_score_per_fold = []\n",
        "  for train, test in kfold.split(spectrograms_shuffled_downscaled,spectrograms_label_shuffled):   \n",
        "    randomlist_for_validation_indx = test\n",
        "    randomlist_for_train_indx = train\n",
        "    # test data\n",
        "    spectrograms_shuffled_downscaled_test = spectrograms_shuffled_downscaled[randomlist_for_validation_indx,:,:,:]\n",
        "    spectrograms_label_shuffled_test = spectrograms_label_shuffled[randomlist_for_validation_indx,:]\n",
        "    #train data\n",
        "    spectrograms_shuffled_downscaled_train = spectrograms_shuffled_downscaled[randomlist_for_train_indx,:,:,:]\n",
        "    spectrograms_label_shuffled_train = spectrograms_label_shuffled[randomlist_for_train_indx,:]\n",
        "      # ---------------- Split labels to equal them during augmentation ----------------\n",
        "    (augmented_image_fast,augmented_image_slow,spectrograms_fast_label,spectrograms_slow_label,validation_spectrograms,validation_labels)  = split_and_augmentation_of_training(spectrograms_shuffled_downscaled_train,spectrograms_label_shuffled_train, augmentation_enable)\n",
        "    # ---------------- Concat Fast and Slow ----------------\n",
        "    augmented_image = np.concatenate((augmented_image_fast,augmented_image_slow),axis=0)\n",
        "    spectrograms_label_shuffled_concat = np.concatenate((spectrograms_fast_label,spectrograms_slow_label),axis=0)\n",
        "\n",
        "    (spectrograms_shuffled_downscaled_test, validation_spectrograms, augmented_image) = normalize_inputs(spectrograms_shuffled_downscaled_test, validation_spectrograms, augmented_image, normalize_inputs_enable)\n",
        "\n",
        "    # ---------------- Neural Network Architecture ----------------\n",
        "    model = Sequential()\n",
        "\n",
        "    initializer = tf.keras.initializers.HeNormal()\n",
        "\n",
        "    model.add(Conv2D(4, (3, 3), input_shape=augmented_image.shape[1:],kernel_initializer=initializer))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(dropout_prob_cnn))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(8, (3, 3), input_shape=augmented_image.shape[1:],kernel_initializer=initializer))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(dropout_prob_cnn))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # model.add(Conv2D(16, (3, 3),kernel_initializer=initializer))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Activation('relu'))\n",
        "    # model.add(Dropout(dropout_prob_cnn))\n",
        "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "    # model.add(Conv2D(64, (3, 3)))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Activation('relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())  \n",
        "    model.add(Dense(32,kernel_initializer=initializer))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(dropout_prob_dense))\n",
        "\n",
        "\n",
        "    model.add(Dense(1,kernel_initializer=initializer))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    print(model.summary())\n",
        "    # ---------------- Compile and Fit ----------------\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='rmsprop',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    earlyStopping = EarlyStopping(monitor='val_loss', patience=35, verbose=0,restore_best_weights=True, mode='min')\n",
        "    # earlyStopping = EarlyStopping(monitor='val_accuracy', patience=15, verbose=0,restore_best_weights=True, mode='max')\n",
        "    history = model.fit((augmented_image),(spectrograms_label_shuffled_concat),\n",
        "                    epochs=epoch_number,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle = True,\n",
        "                    callbacks=[earlyStopping],\n",
        "                    validation_data = ((validation_spectrograms) , (validation_labels)))\n",
        "    tf.keras.models.load_model\n",
        "    test_loss, test_accuracy  = model.evaluate((spectrograms_shuffled_downscaled_test),(spectrograms_label_shuffled_test),\n",
        "                  batch_size=batch_size)\n",
        "    # ---------------- Get Test Results ----------------\n",
        "    y_test_predicted = model.predict((spectrograms_shuffled_downscaled_test), batch_size=batch_size)\n",
        "    # ----- Binarize y_test_predicted values -----\n",
        "    y_test_predicted_binary = np.zeros(y_test_predicted.size)\n",
        "    for ii in range(y_test_predicted.size):\n",
        "      if y_test_predicted[ii] < 0.5:\n",
        "        y_test_predicted_binary[ii] = 0\n",
        "      else:\n",
        "        y_test_predicted_binary[ii] = 1\n",
        "    \n",
        "    test_precision, test_recall, test_f1_score, support = precision_recall_fscore_support(spectrograms_label_shuffled_test, y_test_predicted_binary, average='macro')\n",
        "\n",
        "    test_accuracy_per_fold.append(test_accuracy)\n",
        "    f1_score_per_fold.append(test_f1_score)\n",
        "\n",
        "  test_accuracy_per_run.append(sum(test_accuracy_per_fold)/num_folds)\n",
        "  f1_score_per_run.append(sum(f1_score_per_fold)/num_folds)\n",
        "print(f'Mean test accuracy is {\"{:.2f}\".format(sum(test_accuracy_per_run)/number_of_repeat)}, mean test f1 score is {\"{:.2f}\".format(sum(f1_score_per_run)/number_of_repeat)}, \\\n",
        "max test accuracy is {\"{:.2f}\".format(max(test_accuracy_per_run))}, max test f1 score is {\"{:.2f}\".format(max(f1_score_per_run))}, \\\n",
        "min test accuracy is {\"{:.2f}\".format(min(test_accuracy_per_run))}, min test f1 score is {\"{:.2f}\".format(min(f1_score_per_run))}, \\\n",
        "std of test accuracy is {\"{:.2f}\".format(np.std(test_accuracy_per_run, axis=0))}, std of test f1 score is {\"{:.2f}\".format(np.std(f1_score_per_run, axis=0))}')\n",
        "elapsed = time.time() - t\n",
        "print(f'Time elapsed through all process: {\"{:.2f}\".format(elapsed)}, sec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N7v4PDP4Dv2",
        "outputId": "5951d564-a5b5-462e-9015-b48812b3b55b"
      },
      "source": [
        "print(f'Mean test accuracy is {\"{:.2f}\".format(sum(test_accuracy_per_run)/number_of_repeat)}, mean test f1 score is {\"{:.2f}\".format(sum(f1_score_per_run)/number_of_repeat)}, \\\n",
        "max test accuracy is {\"{:.2f}\".format(max(test_accuracy_per_run))}, max test f1 score is {\"{:.2f}\".format(max(f1_score_per_run))}, \\\n",
        "min test accuracy is {\"{:.2f}\".format(min(test_accuracy_per_run))}, min test f1 score is {\"{:.2f}\".format(min(f1_score_per_run))}, \\\n",
        "std of test accuracy is {\"{:.2f}\".format(np.std(test_accuracy_per_run, axis=0))}, std of test f1 score is {\"{:.2f}\".format(np.std(f1_score_per_run, axis=0))}')\n",
        "print(f'Time elapsed through all process: {\"{:.2f}\".format(elapsed)}, sec')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean test accuracy is 0.87, mean test f1 score is 0.84, max test accuracy is 0.89, max test f1 score is 0.86, min test accuracy is 0.85, min test f1 score is 0.82, std of test accuracy is 0.01, std of test f1 score is 0.02\n",
            "Time elapsed through all process: 662.07, sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJLXLhf3XGID"
      },
      "source": [
        "# Test Amaçlı\n",
        "model.predict(augmented_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cLwiyFcRdJl"
      },
      "source": [
        "# Test Amaçlı\n",
        "model.predict(spectrograms_shuffled_downscaled_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}