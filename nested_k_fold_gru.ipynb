{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nested_k_fold_gru.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6pKqrggN8RZ4wwJfOhNkt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertege/FMCW-Data-Classification-/blob/main/nested_k_fold_gru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_Rv6KprFdKu"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "import scipy.io\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Flatten,Input, Lambda, Activation, Concatenate, LSTM, Embedding, Bidirectional, GRU, BatchNormalization, LayerNormalization, Normalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow import keras\n",
        "import time\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "import random\n",
        "from numpy.random import seed\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48S8qT3aFjeM",
        "outputId": "60733a03-66e7-4600-fee2-23f4d03b1f79"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sSad3_jFkBN"
      },
      "source": [
        "training_and_val_data = scipy.io.loadmat('/content/drive/MyDrive/jittered_data.mat')  \n",
        "training_and_val_label = scipy.io.loadmat('/content/drive/MyDrive/jittered_label.mat')\n",
        "# Convert mat. data to ndarray\n",
        "training_and_validation_dataset = training_and_val_data['training_data']  \n",
        "training_and_validation_label = training_and_val_label['training_label']  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRuHLpB0FpkK"
      },
      "source": [
        "X_1 = training_and_validation_dataset[:,1,:]\n",
        "X_2 = training_and_validation_dataset[:,2,:]\n",
        "X_3 = training_and_validation_dataset[:,3,:]\n",
        "X_4 = training_and_validation_dataset[:,4,:]\n",
        "X_5 = training_and_validation_dataset[:,5,:]\n",
        "X_6 = training_and_validation_dataset[:,6,:]\n",
        "X_7 = training_and_validation_dataset[:,7,:]\n",
        "X_8 = training_and_validation_dataset[:,8,:]\n",
        "X_9 = training_and_validation_dataset[:,9,:]\n",
        "X_10 = training_and_validation_dataset[:,10,:]\n",
        "X_11 = training_and_validation_dataset[:,11,:]\n",
        "X_12 = training_and_validation_dataset[:,12,:]\n",
        "X_13 = training_and_validation_dataset[:,13,:]\n",
        "X_14 = training_and_validation_dataset[:,14,:]\n",
        "X_15 = training_and_validation_dataset[:,0,:]\n",
        "\n",
        "y_label = training_and_validation_label[:,0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dEr3tx9WYfI"
      },
      "source": [
        "# FMCW Learning with Function API GRU\n",
        "\n",
        "# seed(2)\n",
        "# tf.random.set_seed(4)\n",
        "\n",
        "from keras import backend as K\n",
        "t = time.time()\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "    \n",
        "# ---------------- Parameters ----------------\n",
        "LSTM_hidden_units = 8\n",
        "dropout_probability = 0.2\n",
        "dense_input_unit = LSTM_hidden_units*14\n",
        "top_words = 4225\n",
        "max_review_length = 4225\n",
        "input_size = 4225\n",
        "embedding_vecor_length = 16\n",
        "epoch_number = 30\n",
        "batch_size = 8\n",
        "dense_unit_number = 8\n",
        "# ---------------- Define the K-fold Cross Validator ----------------\n",
        "size_of_validation = 50\n",
        "num_folds = 5\n",
        "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state = 1) # random_state = 1 ile split run'dan run'a sabit.\n",
        "fold_no = 1\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "def summation(vects):\n",
        "    x, y = vects\n",
        "    return x+y\n",
        "def summation_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1)\n",
        "\n",
        "\n",
        "def create_base_network1(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def create_base_network2(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "def create_base_network3(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def create_base_network4(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "# def create_base_network5(input_shape):\n",
        " \n",
        "#     input = Input(shape=input_shape)\n",
        "#     x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "#     x = (Dropout(dropout_probability))(x)\n",
        "#     x = GRU(LSTM_hidden_units)(x)\n",
        "#     x = (Dropout(dropout_probability))(x)\n",
        "#     return Model(input, x)\n",
        "\n",
        "def create_base_network6(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "def create_base_network7(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def create_base_network8(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "def create_base_network9(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def create_base_network10(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "def create_base_network11(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def create_base_network12(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "def create_base_network13(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def create_base_network14(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def create_base_network15(input_shape):\n",
        " \n",
        "    input = Input(shape=input_shape)\n",
        "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length)(input)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    x = GRU(LSTM_hidden_units)(x)\n",
        "    x = (Dropout(dropout_probability))(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def decoder(input_shape):\n",
        "    input = Input(shape=input_shape)    \n",
        "    x = (Dropout(dropout_probability))(input)\n",
        "    x = Dense(dense_unit_number, activation=\"sigmoid\")(x)        # --  kernel_regularizer='l2'\n",
        "    \n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "    return Model(input, out)\n",
        "\n",
        "\n",
        "for train, test in kfold.split(X_1,y_label):   \n",
        "  randomlist_for_validation_indx = random.sample(range(0, train.size), size_of_validation) # split validation data\n",
        "  randomlist_for_train_indx = np.delete(range(0, train.size), randomlist_for_validation_indx) # split training data\n",
        "  input_shape = ((input_size))\n",
        "  base_network1 = create_base_network1(input_shape)\n",
        "  base_network2 = create_base_network2(input_shape)\n",
        "  base_network3 = create_base_network3(input_shape)\n",
        "  base_network4 = create_base_network4(input_shape)\n",
        "  # base_network5 = create_base_network5(input_shape)\n",
        "  base_network6 = create_base_network6(input_shape)\n",
        "  base_network7 = create_base_network7(input_shape)\n",
        "  base_network8 = create_base_network8(input_shape)\n",
        "  base_network9 = create_base_network9(input_shape)\n",
        "  base_network10 = create_base_network10(input_shape)\n",
        "  base_network11 = create_base_network11(input_shape)\n",
        "  base_network12 = create_base_network12(input_shape)\n",
        "  base_network13 = create_base_network13(input_shape)\n",
        "  base_network14 = create_base_network14(input_shape)\n",
        "  base_network15 = create_base_network15(input_shape)\n",
        "\n",
        "  input_1  = Input(shape=input_shape)\n",
        "  input_2  = Input(shape=input_shape)\n",
        "  input_3  = Input(shape=input_shape)\n",
        "  input_4  = Input(shape=input_shape)\n",
        "  # input_5  = Input(shape=input_shape)\n",
        "  input_6  = Input(shape=input_shape)\n",
        "  input_7  = Input(shape=input_shape)\n",
        "  input_8  = Input(shape=input_shape)\n",
        "  input_9  = Input(shape=input_shape)\n",
        "  input_10 = Input(shape=input_shape)\n",
        "  input_11 = Input(shape=input_shape)\n",
        "  input_12 = Input(shape=input_shape)\n",
        "  input_13 = Input(shape=input_shape)\n",
        "  input_14 = Input(shape=input_shape)\n",
        "  input_15 = Input(shape=input_shape)\n",
        "\n",
        "  processed_1  = base_network1(input_1 )\n",
        "  processed_2  = base_network2(input_2 )\n",
        "  processed_3  = base_network1(input_3 )\n",
        "  processed_4  = base_network2(input_4 )\n",
        "  # processed_5  = base_network1(input_5 )\n",
        "  processed_6  = base_network2(input_6 )\n",
        "  processed_7  = base_network1(input_7 )\n",
        "  processed_8  = base_network2(input_8 )\n",
        "  processed_9  = base_network1(input_9 )\n",
        "  processed_10 = base_network2(input_10)\n",
        "  processed_11 = base_network1(input_11)\n",
        "  processed_12 = base_network2(input_12)\n",
        "  processed_13 = base_network1(input_13)\n",
        "  processed_14 = base_network2(input_14)\n",
        "  processed_15 = base_network1(input_15)\n",
        "  concat_lstm_outputs = Concatenate()([processed_1, processed_2,processed_3, processed_4, processed_6,processed_7, processed_8,processed_9, processed_10,processed_11, processed_12,processed_13, processed_14,processed_15])\n",
        "  \n",
        "  base_decoder_network = decoder((dense_input_unit))\n",
        "  out1 = base_decoder_network(concat_lstm_outputs)\n",
        "\n",
        "  model = Model(inputs=[input_1, input_2,input_3, input_4, input_6,input_7, input_8,input_9, input_10,input_11, input_12,input_13, input_14,input_15], outputs=[out1])       \n",
        "\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "\n",
        "  model.summary()\n",
        "  earlyStopping = EarlyStopping(monitor='val_loss', patience=6, verbose=0,restore_best_weights=True, mode='min')\n",
        "\n",
        "  filepath_checkpoint = \"mymodel\"\n",
        "  model_checkpoint_callback =  keras.callbacks.ModelCheckpoint(\n",
        "          filepath= filepath_checkpoint,\n",
        "          save_weights_only=True,\n",
        "          mode=\"min\",\n",
        "          save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "          monitor=\"val_loss\",\n",
        "          verbose=1,\n",
        "      )\n",
        "\n",
        "  history = model.fit((X_1[train][randomlist_for_train_indx,:] ,X_2[train][randomlist_for_train_indx,:] ,X_3[train][randomlist_for_train_indx,:] ,X_4[train][randomlist_for_train_indx,:]  ,X_6[train][randomlist_for_train_indx,:],X_7[train][randomlist_for_train_indx,:] ,X_8[train][randomlist_for_train_indx,:] ,\n",
        "                  X_9[train][randomlist_for_train_indx,:] ,X_10[train][randomlist_for_train_indx,:],X_11[train][randomlist_for_train_indx,:],X_12[train][randomlist_for_train_indx,:],X_13[train][randomlist_for_train_indx,:],X_14[train][randomlist_for_train_indx,:],X_15[train][randomlist_for_train_indx,:]), (y_label[train][randomlist_for_train_indx]),\n",
        "                  epochs=epoch_number,\n",
        "                  batch_size=batch_size,\n",
        "                  callbacks=[earlyStopping, model_checkpoint_callback],\n",
        "                  validation_data=((X_1[train][randomlist_for_validation_indx,:] ,X_2[train][randomlist_for_validation_indx,:],X_3[train][randomlist_for_validation_indx,:] ,X_4[train][randomlist_for_validation_indx,:],X_6[train][randomlist_for_validation_indx,:],X_7[train][randomlist_for_validation_indx,:] ,X_8[train][randomlist_for_validation_indx,:],X_9[train][randomlist_for_validation_indx,:] ,\n",
        "                  X_10[train][randomlist_for_validation_indx,:],X_11[train][randomlist_for_validation_indx,:] ,X_12[train][randomlist_for_validation_indx,:],X_13[train][randomlist_for_validation_indx,:] ,X_14[train][randomlist_for_validation_indx,:],X_15[train][randomlist_for_validation_indx,:]), (y_label[train][randomlist_for_validation_indx])))\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  tf.keras.models.load_model\n",
        "  test_loss, test_accuracy = model.evaluate((X_1[test],X_2[test],X_3[test],X_4[test],\n",
        "                                                              X_6[test],X_7[test],X_8[test],X_9[test],X_10[test],X_11[test],\n",
        "                                                              X_12[test],X_13[test],X_14[test],X_15[test]) ,y_label[test], batch_size=8)\n",
        "  \n",
        "  y_test_predicted = model.predict((X_1[test],X_2[test],X_3[test],X_4[test],\n",
        "                                                              X_6[test],X_7[test],X_8[test],X_9[test],X_10[test],X_11[test],\n",
        "                                                              X_12[test],X_13[test],X_14[test],X_15[test]), batch_size=8)\n",
        "  # ----- Binarize y_test_predicted values -----\n",
        "  y_test_predicted_binary = np.zeros(y_test_predicted.size)\n",
        "  for ii in range(y_test_predicted.size):\n",
        "    if y_test_predicted[ii] < 0.5:\n",
        "      y_test_predicted_binary[ii] = 0\n",
        "    else:\n",
        "      y_test_predicted_binary[ii] = 1\n",
        "\n",
        "  # ----- Get precision, recall and f1 score -----\n",
        "  test_precision, test_recall, test_f1_score, support = precision_recall_fscore_support(y_label[test], y_test_predicted_binary, average='macro')\n",
        "  # ----- Print Score -----\n",
        "  print(f'Score for fold {fold_no}: loss is {test_loss}, accuracy is {test_accuracy}, f1_score is {test_f1_score}, precision is {test_precision}, recall is {test_recall}')\n",
        "  acc_per_fold.append(test_accuracy)\n",
        "  f1_score_per_fold.append(test_f1_score)\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "print(sum(acc_per_fold)/num_folds)\n",
        "print(sum(f1_score_per_fold)/num_folds)\n",
        "elapsed = time.time() - t\n",
        "print(elapsed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUXkuIXnKzOJ"
      },
      "source": [
        "print(y_test_predicted) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}